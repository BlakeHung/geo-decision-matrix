import json
import os
from typing import List, Dict
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from fastapi.middleware.cors import CORSMiddleware

# LangChain Imports
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# åˆå§‹åŒ– FastAPI
app = FastAPI(
    title="Geo-Decision Matrix AI",
    description="Based on Spark Analysis of 1M+ Logistics Datapoints",
    version="2.0.0"
)

# å…è¨±è·¨åŸŸ (è®“ä½ çš„ React å‰ç«¯å¯ä»¥å‘¼å«)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# 1. è¼‰å…¥ Spark åˆ†æå¾Œçš„çŸ¥è­˜åº« (The Knowledge)
# ==========================================
def load_risk_knowledge_base() -> str:
    """
    è®€å–ç”± PySpark ETL æµç¨‹ç”¢å‡ºçš„åœ°ç†èª¤å·®çµ±è¨ˆæ•¸æ“šã€‚
    é€™äº›æ•¸æ“šå·²ç¶“å»è­˜åˆ¥åŒ–ï¼Œåƒ…åŒ…å«ï¼šåŸå¸‚ã€å¹³å‡èª¤å·®è·é›¢(ç±³)ã€é«˜é¢¨éšªå–®é‡ä½”æ¯”ã€‚
    """
    file_path = "data/map_risk_summary.json"
    
    # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ (æœ¬åœ°æ¸¬è©¦é‚„æ²’è·‘ Spark)ï¼Œå…ˆçµ¦ä¸€å€‹ Mock æ•¸æ“š
    if not os.path.exists(file_path):
        return json.dumps([
            {"city": "Taipei", "avg_error_meters": 8.5, "risk_rate": 0.02, "verdict": "SAFE"},
            {"city": "Changhua", "avg_error_meters": 145.2, "risk_rate": 0.35, "verdict": "RISKY"},
            {"city": "Kaohsiung", "avg_error_meters": 25.1, "risk_rate": 0.12, "verdict": "MONITOR"},
        ], indent=2)

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            # å°‡ JSON è½‰ç‚ºå­—ä¸²ï¼Œæº–å‚™æ³¨å…¥ Prompt
            return json.dumps(data, ensure_ascii=False, indent=2)
    except Exception as e:
        return f"Error loading data: {str(e)}"

# é å…ˆè¼‰å…¥æ•¸æ“š (æ¨¡æ“¬ RAG çš„ Retrieve éšæ®µ)
RISK_CONTEXT = load_risk_knowledge_base()

# ==========================================
# 2. è¨­å®š AI æ¨¡å‹èˆ‡ Prompt (The Brain)
# ==========================================
# ä½¿ç”¨æœ¬åœ°é‹è¡Œçš„ Llama 3
llm = Ollama(model="llama3", base_url="http://host.docker.internal:11434")

# å®šç¾© "Staff Engineer" ç­‰ç´šçš„ System Prompt
# é—œéµï¼šè³¦äºˆå®ƒã€ŒæŠ€è¡“æ±ºç­–é¡§å•ã€çš„è§’è‰²ï¼Œä¸¦è¨­å®šåš´æ ¼çš„é¢¨éšªæ¨™æº–
template = """
You are a Senior Technical Decision Advisor for a large-scale logistics platform.
We are evaluating replacing our current Map Provider (Provider A) with a cost-effective alternative (Provider B).

I will provide you with a statistical summary generated by our Spark cluster, which analyzed 1 million historical delivery coordinates.
The data compares the geolocation difference between Provider A and Provider B.

### Risk Definition:
- **Low Risk**: Avg error < 20m, Risk Rate < 5% (Safe to switch)
- **Medium Risk**: Avg error 20-50m (Need manual review)
- **High Risk**: Avg error > 50m OR Risk Rate > 20% (Do NOT switch)

### Data Summary (from Spark):
{risk_context}

### User Question:
{question}

---
### Instructions:
1. Analyze the specific city or region mentioned in the question based on the Data Summary.
2. Provide a clear **"Go / No-Go"** recommendation.
3. Use professional, data-driven language (e.g., "Based on the 35% risk variance...").
4. **Do NOT** mention any specific company names (like Nidin, Uber, etc.). Refer to us as "the Platform".
5. Answer in **Traditional Chinese (ç¹é«”ä¸­æ–‡)**.
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["question"],
    partial_variables={"risk_context": RISK_CONTEXT} # é å…ˆæ³¨å…¥ Spark æ•¸æ“š
)

# å»ºç«‹è™•ç†éˆ: Prompt -> LLM -> String Output
chain = prompt | llm | StrOutputParser()

# ==========================================
# 3. å®šç¾© API ä»‹é¢
# ==========================================
class AnalysisRequest(BaseModel):
    query: str = Field(..., description="E.g., è«‹å•å½°åŒ–åœ°å€é©åˆåˆ‡æ›åœ°åœ–å•†å—ï¼Ÿ")

class AnalysisResponse(BaseModel):
    analysis: str
    data_source_timestamp: str = "2025-12-19 (Spark Job #9527)"

@app.post("/api/v1/consult", response_model=AnalysisResponse)
async def consult_ai(request: AnalysisRequest):
    """
    AI æ±ºç­–é¡§å•æ¥å£
    æ¥æ”¶è‡ªç„¶èªè¨€å•é¡Œï¼Œå›å‚³åŸºæ–¼æ•¸æ“šçš„æ±ºç­–å»ºè­°ã€‚
    """
    print(f"ğŸ¤” Thinking about: {request.query}")
    try:
        # å‘¼å« Llama 3
        result = chain.invoke({"question": request.query})
        return AnalysisResponse(analysis=result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
def health_check():
    return {"status": "active", "model": "Llama 3", "data_source": "Spark ETL"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)