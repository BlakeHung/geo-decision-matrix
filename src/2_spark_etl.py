from pyspark.sql import SparkSession
from pyspark.sql.functions import col, acos, cos, sin, lit, avg, count, when
import math
import json
import os

# å•Ÿå‹• Spark (å–®æ©Ÿæ¨¡å¼)
spark = SparkSession.builder \
    .appName("GeoRiskETL") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

print("ğŸ”¥ Spark Session å•Ÿå‹•æˆåŠŸï¼Œé–‹å§‹ ETL æµç¨‹...")

# 1. Extract
df = spark.read.csv("data/raw_addresses.csv", header=True, inferSchema=True)

# 2. Transform: è¨ˆç®— Haversine Distance (å…¬å°º)
# åœ°çƒåŠå¾‘ R = 6371 km
df = df.withColumn("error_m", 
    acos(
        sin(col("g_lat")*math.pi/180) * sin(col("m_lat")*math.pi/180) + 
        cos(col("g_lat")*math.pi/180) * cos(col("m_lat")*math.pi/180) * cos((col("m_lng")-col("g_lng"))*math.pi/180)
    ) * 6371 * 1000
)

# å®šç¾©é¢¨éšªå–®ï¼šèª¤å·® > 50m
df = df.withColumn("is_risky", when(col("error_m") > 50, 1).otherwise(0))

# èšåˆçµ±è¨ˆ
result = df.groupBy("city").agg(
    avg("error_m").alias("avg_error"),
    (pd_sum := avg("is_risky")).alias("risk_rate") # avg(0/1) å³ç‚ºæ¯”ä¾‹
).orderBy("risk_rate", ascending=False)

# 3. Load: é¡¯ç¤ºä¸¦å„²å­˜çµæœ
print("ğŸ“Š å„åŸå¸‚åœ°åœ–é¢¨éšªåˆ†æå ±å‘Šï¼š")
result.show()

# è½‰å­˜ JSON çµ¦ AI ä½¿ç”¨
summary = [row.asDict() for row in result.collect()]
json_path = "data/risk_summary.json"
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(summary, f, indent=2)

print(f"âœ… ETL å®Œæˆï¼ŒçŸ¥è­˜åº«å·²æ›´æ–°ï¼š{json_path}")
spark.stop()