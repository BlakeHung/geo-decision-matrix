from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType
import math
import os
import time

# è¨­å®šè¼¸å…¥èˆ‡è¼¸å‡ºè·¯å¾‘
INPUT_FILE = 'data/gps_tracks.csv'
OUTPUT_PARQUET = 'data/cleaned_data.parquet'

def calculate_haversine(lat1, lon1, lat2, lon2):
    """
    è¨ˆç®—åœ°çƒè¡¨é¢å…©é»ä¹‹é–“çš„è·é›¢ (km)
    é€™æ˜¯ CPU æ“…é•·çš„è¤‡é›œé‚è¼¯é‹ç®—
    """
    if None in [lat1, lon1, lat2, lon2]:
        return 0.0
    
    R = 6371  # åœ°çƒåŠå¾‘ (km)
    
    # å°‡è§’åº¦è½‰ç‚ºå¼§åº¦
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    
    a = math.sin(dlat/2)**2 + \
        math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * \
        math.sin(dlon/2)**2
        
    a = min(1.0, max(0.0, a)) # é˜²ç¦¦æ€§ç·¨ç¨‹ï¼šé˜²æ­¢æµ®é»æ•¸èª¤å·®
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    
    return R * c

def run_spark_etl():
    print("ğŸš€ [Stage 1] Spark ETL å•Ÿå‹•...")
    start_time = time.time()

    # 1. åˆå§‹åŒ– Spark Session (åŠ ä¸Šè¨˜æ†¶é«”é™åˆ¶ï¼Œé˜²æ­¢ OOM)
    # å‡è¨­ä½ çš„ä¸»æ©Ÿæœ‰ 32GB RAMï¼Œé€™è£¡æˆ‘å€‘çµ¦ Spark 8GB å †å…§ + 2GB å †å¤–
    spark = SparkSession.builder \
        .appName("GeoDecisionMatrix_ETL") \
        .config("spark.driver.memory", "8g") \
        .config("spark.executor.memory", "8g") \
        .config("spark.memory.offHeap.enabled", "true") \
        .config("spark.memory.offHeap.size", "2g") \
        .getOrCreate()

    # è¨»å†Š UDF
    haversine_udf = F.udf(calculate_haversine, DoubleType())

    # 2. è®€å– CSV (Row-based, æ…¢)
    print(f"ğŸ“¥ æ­£åœ¨è®€å– CSV: {INPUT_FILE}")
    df = spark.read.csv(INPUT_FILE, header=True, inferSchema=True)

    # 3. å®šç¾© Window (æŒ‰ä½¿ç”¨è€…åˆ†çµ„ï¼ŒæŒ‰æ™‚é–“æ’åº)
    # é€™æ˜¯ GPU æœ€æ€•çš„é‚è¼¯ (å‰å¾Œä¾è³´)ï¼Œäº¤çµ¦ CPU åš
    window_spec = Window.partitionBy("user_id").orderBy("timestamp")

    # 4. è³‡æ–™æ¸…æ´—é‚è¼¯
    print("âš™ï¸ åŸ·è¡Œè¤‡é›œæ¸…æ´—é‚è¼¯ (Lag + Haversine)...")
    
    # å–å¾—å‰ä¸€ç­†çš„ç¶“ç·¯åº¦ (Lag)
    df_lag = df.withColumn("prev_lat", F.lag("latitude").over(window_spec)) \
               .withColumn("prev_lon", F.lag("longitude").over(window_spec))

    # è¨ˆç®—è·é›¢ (ä½¿ç”¨ UDF)
    df_detailed = df_lag.withColumn("step_distance_km", 
        haversine_udf("prev_lat", "prev_lon", "latitude", "longitude")
    ).na.fill(0.0, subset=["step_distance_km"])

    # æ¨™è¨˜ç•°å¸¸åœæ»¯ (Stuck)
    df_clean = df_detailed.withColumn("is_stuck", 
        F.when(F.col("step_distance_km") == 0.0, 1).otherwise(0)
    )

    # 5. å¯«å…¥ Parquet (Column-based, å¿«)
    # é€™æ˜¯ GPU å–œæ­¡çš„æ ¼å¼ï¼šé›¶è§£ææˆæœ¬ã€åˆä½µè¨˜æ†¶é«”è®€å–
    print(f"ğŸ’¾ æ­£åœ¨å¯«å…¥ Parquet: {OUTPUT_PARQUET}")
    
    # coalesce(1) æ˜¯ç‚ºäº†å¼·åˆ¶è¼¸å‡ºæˆå–®ä¸€æª”æ¡ˆ (æ–¹ä¾¿å±•ç¤º)ï¼Œç”Ÿç”¢ç’°å¢ƒé€šå¸¸ä¸é€™æ¨£åš
    df_clean.write.parquet(
        OUTPUT_PARQUET, 
        mode="overwrite", 
        compression="snappy"
    )

    elapsed = time.time() - start_time
    print(f"âœ… [Stage 1] Spark ETL å®Œæˆï¼è€—æ™‚: {elapsed:.2f} ç§’")
    
    spark.stop()

if __name__ == "__main__":
    run_spark_etl()