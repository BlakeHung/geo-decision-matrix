import os
import time
import numpy as np
import pandas as pd
import warnings

# --- ğŸ”¥ [é—œéµä¿®æ­£] WSL 2 é›™å¡æ•‘å‘½ä¸¹ ---
# é€™äº›è¨­å®šå‘Šè¨´ Dask/NCCLï¼š
# "æˆ‘å€‘åœ¨ WSL è™›æ“¬æ©Ÿè£¡ï¼Œä¸è¦å˜—è©¦ P2Pï¼Œä¸è¦ç”¨ InfiniBandï¼Œå…¨éƒ¨èµ° TCP æ…¢è»Šé“ï¼"

# 1. æ ¸å¿ƒè®Šæ•¸ï¼šé—œé–‰ NCCL P2P (è§£æ±º Error 201)
os.environ["NCCL_P2P_DISABLE"] = "1"
os.environ["NCCL_IB_DISABLE"] = "1"  # ç¦ç”¨ InfiniBand

# 2. Dask é€šè¨Šå±¤ï¼šå¼·åˆ¶èµ° TCP
os.environ["DASK_DISTRIBUTED__COMM__DEFAULT_SCHEME"] = "tcp"
os.environ["DASK_DISTRIBUTED__COMM__TIMEOUTS__CONNECT"] = "100s" # æ”¾å¯¬é€£ç·šè¶…æ™‚
os.environ["DASK_CUDA_INTERFACE"] = "eth0" # ç¶å®š WSL è™›æ“¬ç¶²å¡

# 3. UCX åº•å±¤ï¼šå°æ®ºæ‰€æœ‰ç¡¬é«”åŠ é€Ÿï¼Œåªç•™ socket
# é€™æ˜¯è§£æ±º "Segmentation Fault" çš„çµ•å°é—œéµ
os.environ["UCX_TLS"] = "tcp,cuda_copy,sockcm"
os.environ["UCX_SOCKADDR_TLS_PRIORITY"] = "sockcm"
os.environ["UCX_NET_DEVICES"] = "eth0" 

# 4. é—œé–‰å¹²æ“¾é …
os.environ["DASK_CUDA_LEAK_CHECK"] = "0"
os.environ["LIBCUDF_CUFILE_POLICY"] = "OFF" # é—œé–‰ GDS (WSL ä¸æ”¯æ´)

try:
    import cudf
    import dask.dataframe as dd
    import dask_cudf
    from dask.distributed import Client
    from dask_cuda import LocalCUDACluster
    from cuml.dask.cluster import KMeans as DaskKMeans
    RAPIDS_AVAILABLE = True
    print("ğŸš€ [System] RAPIDS æ¨¡çµ„è¼‰å…¥æˆåŠŸï¼")
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¥—ä»¶: {e}")
    exit(1)

# è¨­å®šæª”æ¡ˆè·¯å¾‘
INPUT_FILE = 'data/decision_result.json'
OUTPUT_FILE = 'data/clustered_result.json'

def generate_mock_data(n_rows=10000):
    """ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š"""
    print(f"âš ï¸ ç”Ÿæˆ {n_rows} ç­†æ¨¡æ“¬æ•¸æ“š...")
    df = pd.DataFrame({
        "driver_id": [f"D_{i}" for i in range(n_rows)],
        "total_km": np.random.uniform(5, 50, n_rows),
        "average_speed": np.random.uniform(10, 80, n_rows),
        "stuck_count": np.random.randint(0, 10, n_rows).astype(float)
    })
    return df

def main():
    if not RAPIDS_AVAILABLE: return

    # --- 1. å•Ÿå‹•é›™å¡æŒ‡æ®å®˜ ---
    print("âš¡ æ­£åœ¨åˆå§‹åŒ– Dask CUDA Cluster (WSL 2 å®‰å…¨æ¨¡å¼)...")
    try:
        # é€™è£¡ä¸å‚³ä»»ä½• protocol åƒæ•¸ï¼Œå®Œå…¨ä¾è³´ä¸Šé¢çš„ os.environ
        # é€™æ˜¯æœ€ä¹¾æ·¨çš„å•Ÿå‹•æ–¹å¼
        cluster = LocalCUDACluster(
            rmm_managed_memory=True,   # å¿…é–‹ï¼šWSL è¨˜æ†¶é«”ç®¡ç†
            threads_per_worker=1,
            jit_unspill=False,
            # é¡¯å¼é—œé–‰æ‰€æœ‰é«˜éšåŠŸèƒ½
            enable_tcp_over_ucx=False, 
            enable_infiniband=False,
            enable_nvlink=False,
            enable_rdmacm=False
        )
        client = Client(cluster)
        print(f"âœ… é›™å¡å¢é›†å•Ÿå‹•æˆåŠŸï¼")
        
        workers = client.scheduler_info()['workers']
        print(f"   Workers: {len(workers)} (ç›®æ¨™: 2)")
        
    except Exception as e:
        print(f"âŒ é›™å¡å•Ÿå‹•å¤±æ•—: {e}")
        print("ğŸ’¡ å»ºè­°ï¼šå¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œè«‹å…ˆé‡å•Ÿ Docker å®¹å™¨ä»¥æ¸…é™¤æ®˜ç•™çš„ CUDA Contextã€‚")
        return

    # --- 2. æº–å‚™æ•¸æ“š ---
    start_time = time.time()
    pdf = generate_mock_data(50000) 
    
    # --- 3. å°‡æ•¸æ“šåˆ†ç™¼çµ¦å…©å¼µé¡¯å¡ ---
    print("âš¡ [GPU] æ­£åœ¨å°‡æ•¸æ“šåˆ‡å‰²ä¸¦å‚³é€è‡³ GPU 0 å’Œ GPU 1...")
    
    # Pandas (CPU) -> Dask (CPU) -> Dask cuDF (GPU)
    ddf_cpu = dd.from_pandas(pdf, npartitions=2)
    ddf_gpu = ddf_cpu.map_partitions(cudf.DataFrame.from_pandas)

    features = ['total_km', 'average_speed', 'stuck_count']
    X_dask = ddf_gpu[features]

    # --- 4. é›™å¡åŒæ­¥é‹ç®— ---
    print("âš¡ [GPU] é–‹å§‹ä¸¦è¡Œ K-Means èšé¡...")
    
    kmeans = DaskKMeans(n_clusters=3, random_state=42)
    kmeans.fit(X_dask)
    
    ddf_gpu['cluster'] = kmeans.predict(X_dask)

    # --- 5. å½™æ•´çµæœ ---
    print("âš¡ [Result] æ­£åœ¨å½™æ•´çµæœ...")
    final_df = ddf_gpu.compute().to_pandas()
    
    duration = time.time() - start_time
    print(f"ğŸš€ [Done] é›™å¡é‹ç®—å®Œæˆï¼ç¸½è€—æ™‚: {duration:.4f} ç§’")
    print(f"ğŸ“Š è™•ç†ç¸½ç­†æ•¸: {len(final_df)}")

    client.close()
    cluster.close()

if __name__ == "__main__":
    main()